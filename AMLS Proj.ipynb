{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit, prange\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Download and Extract Dataset\n",
    "# -----------------------------\n",
    "def download_and_extract_movielens(url, extract_path):\n",
    "    zip_path = 'ml-25m.zip'\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(\"Downloading MovieLens 25M dataset...\")\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "        print(\"Download completed.\")\n",
    "    else:\n",
    "        print(\"Zip file already exists.\")\n",
    "        \n",
    "    if not os.path.exists(extract_path):\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall()\n",
    "        print(\"Extraction completed.\")\n",
    "    else:\n",
    "        print(\"Dataset already extracted.\")\n",
    "\n",
    "dataset_url = \"https://files.grouplens.org/datasets/movielens/ml-25m.zip\"\n",
    "data_dir = \"ml-25m\"\n",
    "download_and_extract_movielens(dataset_url, data_dir)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load and Index Data\n",
    "# -----------------------------\n",
    "print(\"Loading ratings data...\")\n",
    "ratings_path = os.path.join(data_dir, \"ratings.csv\")\n",
    "ratings_df = pd.read_csv(ratings_path)\n",
    "\n",
    "print(\"Loading movies data...\")\n",
    "movies_path = os.path.join(data_dir, \"movies.csv\")\n",
    "movies_df = pd.read_csv(movies_path)\n",
    "\n",
    "# Map userId and movieId to indices\n",
    "print(\"Mapping user IDs and movie IDs to indices...\")\n",
    "unique_user_ids = ratings_df['userId'].unique()\n",
    "unique_movie_ids = ratings_df['movieId'].unique()\n",
    "\n",
    "user_id_to_index = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
    "movie_id_to_index = {movie_id: idx for idx, movie_id in enumerate(unique_movie_ids)}\n",
    "\n",
    "num_users = len(unique_user_ids)\n",
    "num_movies = len(unique_movie_ids)\n",
    "\n",
    "print(f\"Number of users: {num_users}\")\n",
    "print(f\"Number of movies: {num_movies}\")\n",
    "\n",
    "# Add index columns\n",
    "ratings_df['user_idx'] = ratings_df['userId'].map(user_id_to_index)\n",
    "ratings_df['movie_idx'] = ratings_df['movieId'].map(movie_id_to_index)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Split Data into Training and Test Sets\n",
    "# -----------------------------\n",
    "print(\"Splitting data into training and test sets...\")\n",
    "train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert training and test data to numpy arrays\n",
    "train_users = train_df['user_idx'].values\n",
    "train_items = train_df['movie_idx'].values\n",
    "train_ratings = train_df['rating'].values\n",
    "\n",
    "test_users = test_df['user_idx'].values\n",
    "test_items = test_df['movie_idx'].values\n",
    "test_ratings = test_df['rating'].values\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Convert Training Data to CSR Matrix (Optional)\n",
    "# -----------------------------\n",
    "# This step is optional since ALS implementation below uses numpy arrays\n",
    "# data_by_user_train = csr_matrix((train_ratings, (train_users, train_items)), shape=(num_users, num_movies))\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Implement ALS with User and Item Biases using Numba\n",
    "# -----------------------------\n",
    "# Initialize parameters\n",
    "num_factors = 0  # No latent factors, only biases\n",
    "lambda_reg = 0.1\n",
    "num_iters = 10\n",
    "global_mean = np.mean(train_ratings)\n",
    "\n",
    "# Initialize user and item biases\n",
    "user_bias = np.zeros(num_users, dtype=np.float32)\n",
    "item_bias = np.zeros(num_movies, dtype=np.float32)\n",
    "\n",
    "# Precompute lists for faster access\n",
    "# For each user, store the indices of items they have rated\n",
    "print(\"Precomputing user-item interactions...\")\n",
    "user_rated_items = [[] for _ in range(num_users)]\n",
    "user_rated_ratings = [[] for _ in range(num_users)]\n",
    "for u, i, r in zip(train_users, train_items, train_ratings):\n",
    "    user_rated_items[u].append(i)\n",
    "    user_rated_ratings[u].append(r)\n",
    "\n",
    "# For each item, store the indices of users who have rated it\n",
    "print(\"Precomputing item-user interactions...\")\n",
    "item_rated_by_users = [[] for _ in range(num_movies)]\n",
    "item_rated_ratings = [[] for _ in range(num_movies)]\n",
    "for u, i, r in zip(train_users, train_items, train_ratings):\n",
    "    item_rated_by_users[i].append(u)\n",
    "    item_rated_ratings[i].append(r)\n",
    "\n",
    "# Convert lists to numpy arrays for Numba compatibility\n",
    "print(\"Converting lists to numpy arrays...\")\n",
    "max_user_rated = max(len(lst) for lst in user_rated_items)\n",
    "max_item_rated = max(len(lst) for lst in item_rated_by_users)\n",
    "\n",
    "user_rated_items_np = np.full((num_users, max_user_rated), -1, dtype=np.int32)\n",
    "user_rated_ratings_np = np.zeros((num_users, max_user_rated), dtype=np.float32)\n",
    "\n",
    "for u in range(num_users):\n",
    "    items = user_rated_items[u]\n",
    "    ratings = user_rated_ratings[u]\n",
    "    user_rated_items_np[u, :len(items)] = items\n",
    "    user_rated_ratings_np[u, :len(ratings)] = ratings\n",
    "\n",
    "item_rated_by_users_np = np.full((num_movies, max_item_rated), -1, dtype=np.int32)\n",
    "item_rated_ratings_np = np.zeros((num_movies, max_item_rated), dtype=np.float32)\n",
    "\n",
    "for i in range(num_movies):\n",
    "    users = item_rated_by_users[i]\n",
    "    ratings = item_rated_ratings[i]\n",
    "    item_rated_by_users_np[i, :len(users)] = users\n",
    "    item_rated_ratings_np[i, :len(ratings)] = ratings\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Define ALS Update Functions with Numba\n",
    "# -----------------------------\n",
    "@njit(parallel=True)\n",
    "def update_user_biases(user_rated_items, user_rated_ratings, global_mean, item_bias, user_bias, lambda_reg, num_users, max_items):\n",
    "    for u in prange(num_users):\n",
    "        sum_ = 0.0\n",
    "        count = 0\n",
    "        for j in range(max_items):\n",
    "            i = user_rated_items[u, j]\n",
    "            if i == -1:\n",
    "                break\n",
    "            sum_ += user_rated_ratings[u, j] - global_mean - item_bias[i]\n",
    "            count += 1\n",
    "        if count > 0:\n",
    "            user_bias[u] = sum_ / (count + lambda_reg)\n",
    "    return user_bias\n",
    "\n",
    "@njit(parallel=True)\n",
    "def update_item_biases(item_rated_by_users, item_rated_ratings, global_mean, user_bias, item_bias, lambda_reg, num_movies, max_users):\n",
    "    for i in prange(num_movies):\n",
    "        sum_ = 0.0\n",
    "        count = 0\n",
    "        for j in range(max_users):\n",
    "            u = item_rated_by_users[i, j]\n",
    "            if u == -1:\n",
    "                break\n",
    "            sum_ += item_rated_ratings[i, j] - global_mean - user_bias[u]\n",
    "            count += 1\n",
    "        if count > 0:\n",
    "            item_bias[i] = sum_ / (count + lambda_reg)\n",
    "    return item_bias\n",
    "\n",
    "@njit\n",
    "def compute_loss(train_users, train_items, train_ratings, global_mean, user_bias, item_bias):\n",
    "    loss = 0.0\n",
    "    for idx in range(len(train_ratings)):\n",
    "        u = train_users[idx]\n",
    "        i = train_items[idx]\n",
    "        pred = global_mean + user_bias[u] + item_bias[i]\n",
    "        err = train_ratings[idx] - pred\n",
    "        loss += err * err\n",
    "    return loss\n",
    "\n",
    "@njit\n",
    "def compute_rmse(train_users, train_items, train_ratings, global_mean, user_bias, item_bias):\n",
    "    mse = 0.0\n",
    "    for idx in range(len(train_ratings)):\n",
    "        u = train_users[idx]\n",
    "        i = train_items[idx]\n",
    "        pred = global_mean + user_bias[u] + item_bias[i]\n",
    "        err = train_ratings[idx] - pred\n",
    "        mse += err * err\n",
    "    return np.sqrt(mse / len(train_ratings))\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Train ALS Model\n",
    "# -----------------------------\n",
    "print(\"Starting ALS training...\")\n",
    "\n",
    "loss_history = []\n",
    "train_rmse_history = []\n",
    "\n",
    "for it in range(num_iters):\n",
    "    # Update user biases\n",
    "    user_bias = update_user_biases(\n",
    "        user_rated_items_np,\n",
    "        user_rated_ratings_np,\n",
    "        global_mean,\n",
    "        item_bias,\n",
    "        user_bias,\n",
    "        lambda_reg,\n",
    "        num_users,\n",
    "        max_user_rated\n",
    "    )\n",
    "    \n",
    "    # Update item biases\n",
    "    item_bias = update_item_biases(\n",
    "        item_rated_by_users_np,\n",
    "        item_rated_ratings_np,\n",
    "        global_mean,\n",
    "        user_bias,\n",
    "        item_bias,\n",
    "        lambda_reg,\n",
    "        num_movies,\n",
    "        max_item_rated\n",
    "    )\n",
    "    \n",
    "    # Compute loss and RMSE\n",
    "    loss = compute_loss(train_users, train_items, train_ratings, global_mean, user_bias, item_bias)\n",
    "    rmse = compute_rmse(train_users, train_items, train_ratings, global_mean, user_bias, item_bias)\n",
    "    \n",
    "    loss_history.append(loss)\n",
    "    train_rmse_history.append(rmse)\n",
    "    \n",
    "    print(f\"Iteration {it+1}/{num_iters} - Loss: {loss:.4f}, Train RMSE: {rmse:.4f}\")\n",
    "\n",
    "print(\"ALS training completed.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Evaluate on Test Set\n",
    "# -----------------------------\n",
    "@njit\n",
    "def compute_rmse_test(test_users, test_items, test_ratings, global_mean, user_bias, item_bias):\n",
    "    mse = 0.0\n",
    "    for idx in range(len(test_ratings)):\n",
    "        u = test_users[idx]\n",
    "        i = test_items[idx]\n",
    "        pred = global_mean + user_bias[u] + item_bias[i]\n",
    "        err = test_ratings[idx] - pred\n",
    "        mse += err * err\n",
    "    return np.sqrt(mse / len(test_ratings))\n",
    "\n",
    "test_rmse = compute_rmse_test(test_users, test_items, test_ratings, global_mean, user_bias, item_bias)\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Plot Loss and RMSE\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_iters + 1), loss_history, marker='o')\n",
    "plt.title('Loss over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_iters + 1), train_rmse_history, marker='o', label='Train RMSE')\n",
    "plt.axhline(y=test_rmse, color='r', linestyle='--', label='Test RMSE')\n",
    "plt.title('RMSE over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Make Recommendations for a Dummy User\n",
    "# -----------------------------\n",
    "print(\"Making recommendations for a dummy user who likes 'The Lord of the Rings: The Fellowship of the Ring'...\")\n",
    "\n",
    "# Identify the movie index for \"The Lord of the Rings: The Fellowship of the Ring\"\n",
    "lotr_title = \"The Lord of the Rings: The Fellowship of the Ring\"\n",
    "lotr_movie = movies_df[movies_df['title'] == lotr_title]\n",
    "if lotr_movie.empty:\n",
    "    print(f\"Movie '{lotr_title}' not found in the dataset.\")\n",
    "else:\n",
    "    lotr_movie_id = lotr_movie.iloc[0]['movieId']\n",
    "    lotr_movie_idx = movie_id_to_index[lotr_movie_id]\n",
    "    \n",
    "    # Create a dummy user\n",
    "    dummy_user_bias = 0.0\n",
    "    dummy_user_ratings = np.zeros(num_movies, dtype=np.float32)\n",
    "    dummy_user_ratings[lotr_movie_idx] = 5.0  # Liked \"The Lord of the Rings: The Fellowship of the Ring\"\n",
    "    \n",
    "    # Predict ratings for all movies for the dummy user\n",
    "    dummy_user_pred = global_mean + dummy_user_bias + item_bias + 0.0  # No user bias since it's a dummy user\n",
    "    dummy_user_pred[lotr_movie_idx] = -np.inf  # Exclude the liked movie from recommendations\n",
    "    \n",
    "    # Get top 10 recommendations\n",
    "    top_n = 10\n",
    "    recommended_indices = np.argpartition(dummy_user_pred, -top_n)[-top_n:]\n",
    "    recommended_indices = recommended_indices[np.argsort(-dummy_user_pred[recommended_indices])]\n",
    "    recommended_movie_ids = [unique_movie_ids[i] for i in recommended_indices]\n",
    "    recommended_movies = movies_df[movies_df['movieId'].isin(recommended_movie_ids)]\n",
    "    \n",
    "    print(\"Top 10 Recommendations:\")\n",
    "    print(recommended_movies[['movieId', 'title']].to_string(index=False))\n",
    "\n",
    "# -----------------------------\n",
    "# 11. Identify Polarizing Movies\n",
    "# -----------------------------\n",
    "print(\"Identifying polarizing movies based on rating variance...\")\n",
    "\n",
    "@njit\n",
    "def compute_rating_variance(ratings, num_movies):\n",
    "    variances = np.zeros(num_movies, dtype=np.float32)\n",
    "    counts = np.zeros(num_movies, dtype=np.int32)\n",
    "    means = np.zeros(num_movies, dtype=np.float32)\n",
    "    for idx in range(len(ratings)):\n",
    "        i = test_items[idx]  # Using test set for demonstration\n",
    "        r = test_ratings[idx]\n",
    "        means[i] += r\n",
    "        counts[i] += 1\n",
    "    for i in prange(num_movies):\n",
    "        if counts[i] > 1:\n",
    "            variances[i] = 0.0\n",
    "            for j in range(len(test_ratings)):\n",
    "                if test_items[j] == i:\n",
    "                    variances[i] += (test_ratings[j] - (means[i]/counts[i]))**2\n",
    "            variances[i] /= (counts[i] - 1)\n",
    "    return variances\n",
    "\n",
    "rating_variances = compute_rating_variance(test_ratings, num_movies)\n",
    "\n",
    "# Get top 10 polarizing movies\n",
    "top_polarizing_indices = np.argsort(-rating_variances)[:10]\n",
    "top_polarizing_movie_ids = [unique_movie_ids[i] for i in top_polarizing_indices]\n",
    "top_polarizing_movies = movies_df[movies_df['movieId'].isin(top_polarizing_movie_ids)]\n",
    "\n",
    "print(\"Top 10 Polarizing Movies:\")\n",
    "print(top_polarizing_movies[['movieId', 'title']].to_string(index=False))\n",
    "\n",
    "# -----------------------------\n",
    "# 12. Addressing the Cold Start Problem with Features (Optional)\n",
    "# -----------------------------\n",
    "# For simplicity, this step is not implemented in this script. Incorporating features would require additional data preprocessing\n",
    "# and modifications to the ALS algorithm to include feature-based regularization or hybrid models.\n",
    "# However, this script provides a foundation upon which such enhancements can be built.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit, prange\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import seaborn as sns\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Download and Extract Dataset\n",
    "# -----------------------------\n",
    "def download_and_extract_movielens(url, extract_path):\n",
    "    zip_path = 'ml-25m.zip'\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(\"Downloading MovieLens 25M dataset...\")\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "        print(\"Download completed.\")\n",
    "    else:\n",
    "        print(\"Zip file already exists.\")\n",
    "        \n",
    "    if not os.path.exists(extract_path):\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall()\n",
    "        print(\"Extraction completed.\")\n",
    "    else:\n",
    "        print(\"Dataset already extracted.\")\n",
    "\n",
    "dataset_url = \"https://files.grouplens.org/datasets/movielens/ml-25m.zip\"\n",
    "data_dir = \"ml-25m\"\n",
    "download_and_extract_movielens(dataset_url, data_dir)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load and Index Data\n",
    "# -----------------------------\n",
    "print(\"Loading ratings data...\")\n",
    "ratings_path = os.path.join(data_dir, \"ratings.csv\")\n",
    "ratings_df = pd.read_csv(ratings_path)\n",
    "\n",
    "print(\"Loading movies data...\")\n",
    "movies_path = os.path.join(data_dir, \"movies.csv\")\n",
    "movies_df = pd.read_csv(movies_path)\n",
    "\n",
    "# Extract genres\n",
    "movies_df['genres'] = movies_df['genres'].apply(lambda x: x.split('|'))\n",
    "\n",
    "# Binarize genres\n",
    "mlb = MultiLabelBinarizer()\n",
    "genre_features = mlb.fit_transform(movies_df['genres'])\n",
    "genre_feature_names = mlb.classes_\n",
    "num_genres = genre_features.shape[1]\n",
    "print(f\"Number of genres: {num_genres}\")\n",
    "\n",
    "# Map userId and movieId to indices\n",
    "print(\"Mapping user IDs and movie IDs to indices...\")\n",
    "unique_user_ids = ratings_df['userId'].unique()\n",
    "unique_movie_ids = ratings_df['movieId'].unique()\n",
    "\n",
    "user_id_to_index = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
    "movie_id_to_index = {movie_id: idx for idx, movie_id in enumerate(unique_movie_ids)}\n",
    "\n",
    "num_users = len(unique_user_ids)\n",
    "num_movies = len(unique_movie_ids)\n",
    "\n",
    "print(f\"Number of users: {num_users}\")\n",
    "print(f\"Number of movies: {num_movies}\")\n",
    "\n",
    "# Add index columns\n",
    "ratings_df['user_idx'] = ratings_df['userId'].map(user_id_to_index)\n",
    "ratings_df['movie_idx'] = ratings_df['movieId'].map(movie_id_to_index)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Split Data into Training and Test Sets\n",
    "# -----------------------------\n",
    "print(\"Splitting data into training and test sets...\")\n",
    "train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert training and test data to numpy arrays\n",
    "train_users = train_df['user_idx'].values.astype(np.int32)\n",
    "train_items = train_df['movie_idx'].values.astype(np.int32)\n",
    "train_ratings = train_df['rating'].values.astype(np.float32)\n",
    "\n",
    "test_users = test_df['user_idx'].values.astype(np.int32)\n",
    "test_items = test_df['movie_idx'].values.astype(np.int32)\n",
    "test_ratings = test_df['rating'].values.astype(np.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Precompute Interactions\n",
    "# -----------------------------\n",
    "print(\"Precomputing user-item and item-user interactions...\")\n",
    "\n",
    "# For ALS with biases and latent factors\n",
    "user_rated_items = [[] for _ in range(num_users)]\n",
    "user_rated_ratings = [[] for _ in range(num_users)]\n",
    "for u, i, r in zip(train_users, train_items, train_ratings):\n",
    "    user_rated_items[u].append(i)\n",
    "    user_rated_ratings[u].append(r)\n",
    "\n",
    "item_rated_by_users = [[] for _ in range(num_movies)]\n",
    "item_rated_ratings = [[] for _ in range(num_movies)]\n",
    "for u, i, r in zip(train_users, train_items, train_ratings):\n",
    "    item_rated_by_users[i].append(u)\n",
    "    item_rated_ratings[i].append(r)\n",
    "\n",
    "# Convert lists to numpy arrays for Numba compatibility\n",
    "print(\"Converting lists to numpy arrays...\")\n",
    "max_user_rated = max(len(lst) for lst in user_rated_items)\n",
    "max_item_rated = max(len(lst) for lst in item_rated_by_users)\n",
    "\n",
    "user_rated_items_np = np.full((num_users, max_user_rated), -1, dtype=np.int32)\n",
    "user_rated_ratings_np = np.zeros((num_users, max_user_rated), dtype=np.float32)\n",
    "\n",
    "for u in range(num_users):\n",
    "    items = user_rated_items[u]\n",
    "    ratings = user_rated_ratings[u]\n",
    "    user_rated_items_np[u, :len(items)] = items\n",
    "    user_rated_ratings_np[u, :len(ratings)] = ratings\n",
    "\n",
    "item_rated_by_users_np = np.full((num_movies, max_item_rated), -1, dtype=np.int32)\n",
    "item_rated_ratings_np = np.zeros((num_movies, max_item_rated), dtype=np.float32)\n",
    "\n",
    "for i in range(num_movies):\n",
    "    users = item_rated_by_users[i]\n",
    "    ratings = item_rated_ratings[i]\n",
    "    item_rated_by_users_np[i, :len(users)] = users\n",
    "    item_rated_ratings_np[i, :len(ratings)] = ratings\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Initialize ALS Parameters\n",
    "# -----------------------------\n",
    "num_factors = 20  # Number of latent factors\n",
    "lambda_reg = 0.1\n",
    "num_iters = 10\n",
    "global_mean = np.mean(train_ratings)\n",
    "\n",
    "# Initialize biases and latent factors\n",
    "user_bias = np.zeros(num_users, dtype=np.float32)\n",
    "item_bias = np.zeros(num_movies, dtype=np.float32)\n",
    "U = np.random.normal(scale=0.1, size=(num_users, num_factors)).astype(np.float32)\n",
    "V = np.random.normal(scale=0.1, size=(num_movies, num_factors)).astype(np.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Define ALS Update Functions with Numba\n",
    "# -----------------------------\n",
    "@njit(parallel=True)\n",
    "def update_user_biases(num_users, max_items, user_rated_items, user_rated_ratings, global_mean, item_bias, U, V, lambda_reg, num_factors):\n",
    "    for u in prange(num_users):\n",
    "        sum_bias = 0.0\n",
    "        count = 0\n",
    "        for j in range(max_items):\n",
    "            i = user_rated_items[u, j]\n",
    "            if i == -1:\n",
    "                break\n",
    "            sum_bias += user_rated_ratings[u, j] - global_mean - item_bias[i] - np.dot(U[u, :], V[i, :])\n",
    "            count += 1\n",
    "        if count > 0:\n",
    "            user_bias[u] = sum_bias / (count + lambda_reg)\n",
    "    return user_bias\n",
    "\n",
    "@njit(parallel=True)\n",
    "def update_item_biases(num_movies, max_users, item_rated_by_users, item_rated_ratings, global_mean, user_bias, U, V, lambda_reg, num_factors):\n",
    "    for i in prange(num_movies):\n",
    "        sum_bias = 0.0\n",
    "        count = 0\n",
    "        for j in range(max_users):\n",
    "            u = item_rated_by_users[i, j]\n",
    "            if u == -1:\n",
    "                break\n",
    "            sum_bias += item_rated_ratings[i, j] - global_mean - user_bias[u] - np.dot(U[u, :], V[i, :])\n",
    "            count += 1\n",
    "        if count > 0:\n",
    "            item_bias[i] = sum_bias / (count + lambda_reg)\n",
    "    return item_bias\n",
    "\n",
    "@njit(parallel=True)\n",
    "def update_U(num_users, max_items, user_rated_items, user_rated_ratings, global_mean, user_bias, V, U, lambda_reg, num_factors):\n",
    "    for u in prange(num_users):\n",
    "        count = 0\n",
    "        A = np.zeros((num_factors, num_factors), dtype=np.float32)\n",
    "        b = np.zeros(num_factors, dtype=np.float32)\n",
    "        for j in range(max_items):\n",
    "            i = user_rated_items[u, j]\n",
    "            if i == -1:\n",
    "                break\n",
    "            r_ui = user_rated_ratings[u, j]\n",
    "            pred = global_mean + user_bias[u] + item_bias[i] + np.dot(U[u, :], V[i, :])\n",
    "            e_ui = r_ui - (global_mean + user_bias[u] + item_bias[i] + np.dot(U[u, :], V[i, :]))\n",
    "            A += V[i, :].reshape(num_factors,1) @ V[i, :].reshape(1, num_factors)\n",
    "            b += V[i, :] * e_ui\n",
    "            count += 1\n",
    "        if count > 0:\n",
    "            A += lambda_reg * np.eye(num_factors, dtype=np.float32)\n",
    "            # Solve for U[u, :]\n",
    "            U[u, :] = np.linalg.solve(A, b)\n",
    "    return U\n",
    "\n",
    "@njit(parallel=True)\n",
    "def update_V(num_movies, max_users, item_rated_by_users, item_rated_ratings, global_mean, item_bias, U, V, lambda_reg, num_factors):\n",
    "    for i in prange(num_movies):\n",
    "        count = 0\n",
    "        A = np.zeros((num_factors, num_factors), dtype=np.float32)\n",
    "        b = np.zeros(num_factors, dtype=np.float32)\n",
    "        for j in range(max_users):\n",
    "            u = item_rated_by_users[i, j]\n",
    "            if u == -1:\n",
    "                break\n",
    "            r_ui = item_rated_ratings[i, j]\n",
    "            pred = global_mean + user_bias[u] + item_bias[i] + np.dot(U[u, :], V[i, :])\n",
    "            e_ui = r_ui - (global_mean + user_bias[u] + item_bias[i] + np.dot(U[u, :], V[i, :]))\n",
    "            A += U[u, :].reshape(num_factors,1) @ U[u, :].reshape(1, num_factors)\n",
    "            b += U[u, :] * e_ui\n",
    "            count += 1\n",
    "        if count > 0:\n",
    "            A += lambda_reg * np.eye(num_factors, dtype=np.float32)\n",
    "            # Solve for V[i, :]\n",
    "            V[i, :] = np.linalg.solve(A, b)\n",
    "    return V\n",
    "\n",
    "@njit\n",
    "def compute_loss(train_users, train_items, train_ratings, global_mean, user_bias, item_bias, U, V):\n",
    "    loss = 0.0\n",
    "    for idx in range(len(train_ratings)):\n",
    "        u = train_users[idx]\n",
    "        i = train_items[idx]\n",
    "        pred = global_mean + user_bias[u] + item_bias[i] + np.dot(U[u, :], V[i, :])\n",
    "        err = train_ratings[idx] - pred\n",
    "        loss += err * err\n",
    "    return loss\n",
    "\n",
    "@njit\n",
    "def compute_rmse(train_users, train_items, train_ratings, global_mean, user_bias, item_bias, U, V):\n",
    "    mse = 0.0\n",
    "    for idx in range(len(train_ratings)):\n",
    "        u = train_users[idx]\n",
    "        i = train_items[idx]\n",
    "        pred = global_mean + user_bias[u] + item_bias[i] + np.dot(U[u, :], V[i, :])\n",
    "        err = train_ratings[idx] - pred\n",
    "        mse += err * err\n",
    "    return np.sqrt(mse / len(train_ratings))\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Train ALS Model with Biases and Latent Factors\n",
    "# -----------------------------\n",
    "print(\"Starting ALS training with biases and latent factors...\")\n",
    "\n",
    "loss_history = []\n",
    "train_rmse_history = []\n",
    "\n",
    "for it in range(num_iters):\n",
    "    # Update user biases\n",
    "    user_bias = update_user_biases(\n",
    "        num_users, max_user_rated, user_rated_items_np, user_rated_ratings_np,\n",
    "        global_mean, item_bias, U, V, lambda_reg, num_factors\n",
    "    )\n",
    "    \n",
    "    # Update item biases\n",
    "    item_bias = update_item_biases(\n",
    "        num_movies, max_item_rated, item_rated_by_users_np, item_rated_ratings_np,\n",
    "        global_mean, user_bias, U, V, lambda_reg, num_factors\n",
    "    )\n",
    "    \n",
    "    # Update user latent factors U\n",
    "    U = update_U(\n",
    "        num_users, max_user_rated, user_rated_items_np, user_rated_ratings_np,\n",
    "        global_mean, user_bias, V, U, lambda_reg, num_factors\n",
    "    )\n",
    "    \n",
    "    # Update item latent factors V\n",
    "    V = update_V(\n",
    "        num_movies, max_item_rated, item_rated_by_users_np, item_rated_ratings_np,\n",
    "        global_mean, item_bias, U, V, lambda_reg, num_factors\n",
    "    )\n",
    "    \n",
    "    # Compute loss and RMSE\n",
    "    loss = compute_loss(train_users, train_items, train_ratings, global_mean, user_bias, item_bias, U, V)\n",
    "    rmse = compute_rmse(train_users, train_items, train_ratings, global_mean, user_bias, item_bias, U, V)\n",
    "    \n",
    "    loss_history.append(loss)\n",
    "    train_rmse_history.append(rmse)\n",
    "    \n",
    "    print(f\"Iteration {it+1}/{num_iters} - Loss: {loss:.4f}, Train RMSE: {rmse:.4f}\")\n",
    "\n",
    "print(\"ALS training with biases and latent factors completed.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Evaluate on Test Set\n",
    "# -----------------------------\n",
    "@njit\n",
    "def compute_rmse_test(test_users, test_items, test_ratings, global_mean, user_bias, item_bias, U, V):\n",
    "    mse = 0.0\n",
    "    for idx in range(len(test_ratings)):\n",
    "        u = test_users[idx]\n",
    "        i = test_items[idx]\n",
    "        pred = global_mean + user_bias[u] + item_bias[i] + np.dot(U[u, :], V[i, :])\n",
    "        err = test_ratings[idx] - pred\n",
    "        mse += err * err\n",
    "    return np.sqrt(mse / len(test_ratings))\n",
    "\n",
    "test_rmse = compute_rmse_test(test_users, test_items, test_ratings, global_mean, user_bias, item_bias, U, V)\n",
    "print(f\"Test RMSE after training: {test_rmse:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Plot Loss and RMSE\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_iters + 1), loss_history, marker='o', color='blue')\n",
    "plt.title('Loss over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_iters + 1), train_rmse_history, marker='o', label='Train RMSE', color='green')\n",
    "plt.axhline(y=test_rmse, color='red', linestyle='--', label='Test RMSE')\n",
    "plt.title('RMSE over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Make Recommendations for a Dummy User\n",
    "# -----------------------------\n",
    "print(\"Making recommendations for a dummy user who likes 'The Lord of the Rings: The Fellowship of the Ring'...\")\n",
    "\n",
    "# Identify the movie index for \"The Lord of the Rings: The Fellowship of the Ring\"\n",
    "lotr_title = \"The Lord of the Rings: The Fellowship of the Ring\"\n",
    "lotr_movie = movies_df[movies_df['title'] == lotr_title]\n",
    "if lotr_movie.empty:\n",
    "    print(f\"Movie '{lotr_title}' not found in the dataset.\")\n",
    "else:\n",
    "    lotr_movie_id = lotr_movie.iloc[0]['movieId']\n",
    "    lotr_movie_idx = movie_id_to_index.get(lotr_movie_id, -1)\n",
    "    \n",
    "    if lotr_movie_idx == -1:\n",
    "        print(f\"Movie ID for '{lotr_title}' not found.\")\n",
    "    else:\n",
    "        # Create a dummy user's predicted ratings\n",
    "        # Assuming the dummy user has no biases (new user), but to incorporate features, we'll handle in next section\n",
    "        # For simplicity, we'll use average ratings plus item biases and latent factors\n",
    "        dummy_user_pred = global_mean + item_bias + np.dot(V, np.zeros(num_factors, dtype=np.float32))\n",
    "        \n",
    "        # Exclude the liked movie from recommendations\n",
    "        dummy_user_pred[lotr_movie_idx] = -np.inf\n",
    "        \n",
    "        # Get top 10 recommendations\n",
    "        top_n = 10\n",
    "        recommended_indices = np.argpartition(dummy_user_pred, -top_n)[-top_n:]\n",
    "        recommended_indices = recommended_indices[np.argsort(-dummy_user_pred[recommended_indices])]\n",
    "        recommended_movie_ids = [unique_movie_ids[i] for i in recommended_indices]\n",
    "        recommended_movies = movies_df[movies_df['movieId'].isin(recommended_movie_ids)]\n",
    "        \n",
    "        print(\"Top 10 Recommendations:\")\n",
    "        print(recommended_movies[['movieId', 'title']].to_string(index=False))\n",
    "\n",
    "# -----------------------------\n",
    "# 11. Identify Polarizing Movies\n",
    "# -----------------------------\n",
    "print(\"Identifying polarizing movies based on rating variance...\")\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_rating_variance(train_users, train_items, train_ratings, num_movies):\n",
    "    variances = np.zeros(num_movies, dtype=np.float32)\n",
    "    counts = np.zeros(num_movies, dtype=np.int32)\n",
    "    means = np.zeros(num_movies, dtype=np.float32)\n",
    "    \n",
    "    # First pass to compute means\n",
    "    for idx in prange(len(train_ratings)):\n",
    "        i = train_items[idx]\n",
    "        r = train_ratings[idx]\n",
    "        means[i] += r\n",
    "        counts[i] += 1\n",
    "    \n",
    "    for i in prange(num_movies):\n",
    "        if counts[i] > 0:\n",
    "            means[i] /= counts[i]\n",
    "    \n",
    "    # Second pass to compute variance\n",
    "    for idx in prange(len(train_ratings)):\n",
    "        i = train_items[idx]\n",
    "        r = train_ratings[idx]\n",
    "        if counts[i] > 1:\n",
    "            variances[i] += (r - means[i]) ** 2\n",
    "    \n",
    "    for i in prange(num_movies):\n",
    "        if counts[i] > 1:\n",
    "            variances[i] /= (counts[i] - 1)\n",
    "        else:\n",
    "            variances[i] = 0.0\n",
    "    \n",
    "    return variances\n",
    "\n",
    "rating_variances = compute_rating_variance(train_users, train_items, train_ratings, num_movies)\n",
    "\n",
    "# Get top 10 polarizing movies\n",
    "top_polarizing_indices = np.argsort(-rating_variances)[:10]\n",
    "top_polarizing_movie_ids = [unique_movie_ids[i] for i in top_polarizing_indices]\n",
    "top_polarizing_movies = movies_df[movies_df['movieId'].isin(top_polarizing_movie_ids)]\n",
    "\n",
    "print(\"Top 10 Polarizing Movies:\")\n",
    "print(top_polarizing_movies[['movieId', 'title']].to_string(index=False))\n",
    "\n",
    "# -----------------------------\n",
    "# 12. ALS with Features Added (Handling Cold Start)\n",
    "# -----------------------------\n",
    "print(\"Starting ALS training with features to handle cold start...\")\n",
    "\n",
    "# Incorporate genre features into item latent factors\n",
    "# Initialize V using genre features\n",
    "# Assuming genres influence item factors, we can set V = genre_features * W, where W is a weight matrix\n",
    "# For simplicity, we'll initialize V with genre features and learn an additional weight matrix\n",
    "\n",
    "# Here, we modify the ALS updates to include item features\n",
    "# This is a simplistic approach; more sophisticated methods can be employed\n",
    "\n",
    "# Let's concatenate genre features with latent factors\n",
    "# New V will have num_factors + num_genres dimensions\n",
    "total_factors = num_factors + num_genres\n",
    "\n",
    "# Initialize new V\n",
    "V_feat = np.random.normal(scale=0.1, size=(num_movies, total_factors)).astype(np.float32)\n",
    "# Incorporate genre features\n",
    "V_feat[:, :num_genres] = genre_features.astype(np.float32) * 0.1  # Scale appropriately\n",
    "\n",
    "# Update functions to handle augmented V\n",
    "@njit(parallel=True)\n",
    "def update_user_biases_feat(num_users, max_items, user_rated_items, user_rated_ratings, global_mean, item_bias, U, V_feat, lambda_reg, num_factors, num_genres):\n",
    "    for u in prange(num_users):\n",
    "        sum_bias = 0.0\n",
    "        count = 0\n",
    "        for j in range(max_items):\n",
    "            i = user_rated_items[u, j]\n",
    "            if i == -1:\n",
    "                break\n",
    "            sum_bias += user_rated_ratings[u, j] - global_mean - item_bias[i] - np.dot(U[u, :], V_feat[i, num_genres:])\n",
    "            count += 1\n",
    "        if count > 0:\n",
    "            user_bias[u] = sum_bias / (count + lambda_reg)\n",
    "    return user_bias\n",
    "\n",
    "@njit(parallel=True)\n",
    "def update_item_biases_feat(num_movies, max_users, item_rated_by_users, item_rated_ratings, global_mean, user_bias, U, V_feat, lambda_reg, num_factors, num_genres):\n",
    "    for i in prange(num_movies):\n",
    "        sum_bias = 0.0\n",
    "        count = 0\n",
    "        for j in range(max_users):\n",
    "            u = item_rated_by_users[i, j]\n",
    "            if u == -1:\n",
    "                break\n",
    "            sum_bias += item_rated_ratings[i, j] - global_mean - user_bias[u] - np.dot(U[u, :], V_feat[i, num_genres:])\n",
    "            count += 1\n",
    "        if count > 0:\n",
    "            item_bias[i] = sum_bias / (count + lambda_reg)\n",
    "    return item_bias\n",
    "\n",
    "@njit(parallel=True)\n",
    "def update_U_feat(num_users, max_items, user_rated_items, user_rated_ratings, global_mean, user_bias, V_feat, U, lambda_reg, num_factors, num_genres):\n",
    "    for u in prange(num_users):\n",
    "        count = 0\n",
    "        A = np.zeros((num_factors, num_factors), dtype=np.float32)\n",
    "        b = np.zeros(num_factors, dtype=np.float32)\n",
    "        for j in range(max_items):\n",
    "            i = user_rated_items[u, j]\n",
    "            if i == -1:\n",
    "                break\n",
    "            r_ui = user_rated_ratings[u, j]\n",
    "            pred = global_mean + user_bias[u] + item_bias[i] + np.dot(U[u, :], V_feat[i, num_genres:])\n",
    "            e_ui = r_ui - pred\n",
    "            A += V_feat[i, num_genres:].reshape(num_factors,1) @ V_feat[i, num_genres:].reshape(1, num_factors)\n",
    "            b += V_feat[i, num_genres:] * e_ui\n",
    "            count += 1\n",
    "        if count > 0:\n",
    "            A += lambda_reg * np.eye(num_factors, dtype=np.float32)\n",
    "            # Solve for U[u, :]\n",
    "            U[u, :] = np.linalg.solve(A, b)\n",
    "    return U\n",
    "\n",
    "@njit(parallel=True)\n",
    "def update_V_feat(num_movies, max_users, item_rated_by_users, item_rated_ratings, global_mean, item_bias, U, V_feat, lambda_reg, num_factors, num_genres):\n",
    "    for i in prange(num_movies):\n",
    "        count = 0\n",
    "        A = np.zeros((num_factors, num_factors), dtype=np.float32)\n",
    "        b = np.zeros(num_factors, dtype=np.float32)\n",
    "        for j in range(max_users):\n",
    "            u = item_rated_by_users[i, j]\n",
    "            if u == -1:\n",
    "                break\n",
    "            r_ui = item_rated_ratings[i, j]\n",
    "            pred = global_mean + user_bias[u] + item_bias[i] + np.dot(U[u, :], V_feat[i, num_genres:])\n",
    "            e_ui = r_ui - pred\n",
    "            A += U[u, :].reshape(num_factors,1) @ U[u, :].reshape(1, num_factors)\n",
    "            b += U[u, :] * e_ui\n",
    "            count += 1\n",
    "        if count > 0:\n",
    "            A += lambda_reg * np.eye(num_factors, dtype=np.float32)\n",
    "            # Solve for V_feat[i, num_genres:]\n",
    "            V_feat[i, num_genres:] = np.linalg.solve(A, b)\n",
    "    return V_feat\n",
    "\n",
    "# -----------------------------\n",
    "# 13. Train ALS Model with Features (Handling Cold Start)\n",
    "# -----------------------------\n",
    "print(\"Starting ALS training with features to handle cold start...\")\n",
    "\n",
    "loss_history_feat = []\n",
    "train_rmse_history_feat = []\n",
    "\n",
    "for it in range(num_iters):\n",
    "    # Update user biases\n",
    "    user_bias = update_user_biases_feat(\n",
    "        num_users, max_user_rated, user_rated_items_np, user_rated_ratings_np,\n",
    "        global_mean, item_bias, U, V_feat, lambda_reg, num_factors, num_genres\n",
    "    )\n",
    "    \n",
    "    # Update item biases\n",
    "    item_bias = update_item_biases_feat(\n",
    "        num_movies, max_item_rated, item_rated_by_users_np, item_rated_ratings_np,\n",
    "        global_mean, user_bias, U, V_feat, lambda_reg, num_factors, num_genres\n",
    "    )\n",
    "    \n",
    "    # Update user latent factors U\n",
    "    U = update_U_feat(\n",
    "        num_users, max_user_rated, user_rated_items_np, user_rated_ratings_np,\n",
    "        global_mean, user_bias, V_feat, U, lambda_reg, num_factors, num_genres\n",
    "    )\n",
    "    \n",
    "    # Update item latent factors V_feat\n",
    "    V_feat = update_V_feat(\n",
    "        num_movies, max_item_rated, item_rated_by_users_np, item_rated_ratings_np,\n",
    "        global_mean, item_bias, U, V_feat, lambda_reg, num_factors, num_genres\n",
    "    )\n",
    "    \n",
    "    # Compute loss and RMSE\n",
    "    loss = compute_loss(train_users, train_items, train_ratings, global_mean, user_bias, item_bias, U, V_feat[:, num_genres:])\n",
    "    rmse = compute_rmse(train_users, train_items, train_ratings, global_mean, user_bias, item_bias, U, V_feat[:, num_genres:])\n",
    "    \n",
    "    loss_history_feat.append(loss)\n",
    "    train_rmse_history_feat.append(rmse)\n",
    "    \n",
    "    print(f\"Feature Iteration {it+1}/{num_iters} - Loss: {loss:.4f}, Train RMSE: {rmse:.4f}\")\n",
    "\n",
    "print(\"ALS training with features completed.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 14. Evaluate on Test Set with Features\n",
    "# -----------------------------\n",
    "@njit\n",
    "def compute_rmse_test_feat(test_users, test_items, test_ratings, global_mean, user_bias, item_bias, U, V_feat, num_genres):\n",
    "    mse = 0.0\n",
    "    for idx in range(len(test_ratings)):\n",
    "        u = test_users[idx]\n",
    "        i = test_items[idx]\n",
    "        pred = global_mean + user_bias[u] + item_bias[i] + np.dot(U[u, :], V_feat[i, num_genres:])\n",
    "        err = test_ratings[idx] - pred\n",
    "        mse += err * err\n",
    "    return np.sqrt(mse / len(test_ratings))\n",
    "\n",
    "test_rmse_feat = compute_rmse_test_feat(test_users, test_items, test_ratings, global_mean, user_bias, item_bias, U, V_feat, num_genres)\n",
    "print(f\"Test RMSE after training with features: {test_rmse_feat:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 15. Plot Loss and RMSE for Feature-Enhanced ALS\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_iters + 1), loss_history_feat, marker='o', color='purple')\n",
    "plt.title('Loss over Iterations with Features')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_iters + 1), train_rmse_history_feat, marker='o', label='Train RMSE with Features', color='orange')\n",
    "plt.axhline(y=test_rmse_feat, color='red', linestyle='--', label='Test RMSE with Features')\n",
    "plt.title('RMSE over Iterations with Features')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 16. Recommendations for a Dummy User with Features\n",
    "# -----------------------------\n",
    "print(\"Making recommendations for a dummy user who likes 'The Lord of the Rings: The Fellowship of the Ring' with features...\")\n",
    "\n",
    "# Identify the movie index for \"The Lord of the Rings: The Fellowship of the Ring\"\n",
    "if lotr_movie.empty:\n",
    "    print(f\"Movie '{lotr_title}' not found in the dataset.\")\n",
    "else:\n",
    "    lotr_movie_id = lotr_movie.iloc[0]['movieId']\n",
    "    lotr_movie_idx = movie_id_to_index.get(lotr_movie_id, -1)\n",
    "    \n",
    "    if lotr_movie_idx == -1:\n",
    "        print(f\"Movie ID for '{lotr_title}' not found.\")\n",
    "    else:\n",
    "        # Create a dummy user's predicted ratings using features\n",
    "        # Assuming the dummy user has no biases (new user), but using item features to predict\n",
    "        # Since we don't have user features, the dummy user is represented by zero vector in U\n",
    "        dummy_U = np.zeros(num_factors, dtype=np.float32)\n",
    "        dummy_user_pred_feat = global_mean + item_bias + np.dot(V_feat[:, num_genres:], dummy_U)\n",
    "        \n",
    "        # Exclude the liked movie from recommendations\n",
    "        dummy_user_pred_feat[lotr_movie_idx] = -np.inf\n",
    "        \n",
    "        # Get top 10 recommendations\n",
    "        top_n = 10\n",
    "        recommended_indices_feat = np.argpartition(dummy_user_pred_feat, -top_n)[-top_n:]\n",
    "        recommended_indices_feat = recommended_indices_feat[np.argsort(-dummy_user_pred_feat[recommended_indices_feat])]\n",
    "        recommended_movie_ids_feat = [unique_movie_ids[i] for i in recommended_indices_feat]\n",
    "        recommended_movies_feat = movies_df[movies_df['movieId'].isin(recommended_movie_ids_feat)]\n",
    "        \n",
    "        print(\"Top 10 Recommendations with Features:\")\n",
    "        print(recommended_movies_feat[['movieId', 'title']].to_string(index=False))\n",
    "\n",
    "# -----------------------------\n",
    "# 17. Identify Polarizing Movies (Optional)\n",
    "# -----------------------------\n",
    "print(\"Identifying polarizing movies based on rating variance...\")\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_rating_variance_train(train_users, train_items, train_ratings, num_movies):\n",
    "    variances = np.zeros(num_movies, dtype=np.float32)\n",
    "    counts = np.zeros(num_movies, dtype=np.int32)\n",
    "    means = np.zeros(num_movies, dtype=np.float32)\n",
    "    \n",
    "    # First pass to compute means\n",
    "    for idx in prange(len(train_ratings)):\n",
    "        i = train_items[idx]\n",
    "        r = train_ratings[idx]\n",
    "        means[i] += r\n",
    "        counts[i] += 1\n",
    "    \n",
    "    for i in prange(num_movies):\n",
    "        if counts[i] > 0:\n",
    "            means[i] /= counts[i]\n",
    "    \n",
    "    # Second pass to compute variance\n",
    "    for idx in prange(len(train_ratings)):\n",
    "        i = train_items[idx]\n",
    "        r = train_ratings[idx]\n",
    "        if counts[i] > 1:\n",
    "            variances[i] += (r - means[i]) ** 2\n",
    "    \n",
    "    for i in prange(num_movies):\n",
    "        if counts[i] > 1:\n",
    "            variances[i] /= (counts[i] - 1)\n",
    "        else:\n",
    "            variances[i] = 0.0\n",
    "    \n",
    "    return variances\n",
    "\n",
    "rating_variances_train = compute_rating_variance_train(train_users, train_items, train_ratings, num_movies)\n",
    "\n",
    "# Get top 10 polarizing movies\n",
    "top_polarizing_indices_train = np.argsort(-rating_variances_train)[:10]\n",
    "top_polarizing_movie_ids_train = [unique_movie_ids[i] for i in top_polarizing_indices_train]\n",
    "top_polarizing_movies_train = movies_df[movies_df['movieId'].isin(top_polarizing_movie_ids_train)]\n",
    "\n",
    "print(\"Top 10 Polarizing Movies:\")\n",
    "print(top_polarizing_movies_train[['movieId', 'title']].to_string(index=False))\n",
    "\n",
    "# Optional: Visualize Polarizing Movies\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='title', y=rating_variances_train[top_polarizing_indices_train], data=top_polarizing_movies_train)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Top 10 Polarizing Movies by Rating Variance')\n",
    "plt.xlabel('Movie Title')\n",
    "plt.ylabel('Rating Variance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 18. Visualize Embeddings and Save as PDFs\n",
    "# -----------------------------\n",
    "print(\"Visualizing user and item embeddings and saving as PDFs...\")\n",
    "\n",
    "# Perform PCA on user embeddings\n",
    "print(\"Performing PCA on user embeddings...\")\n",
    "pca_users = PCA(n_components=2)\n",
    "U_pca = pca_users.fit_transform(U)\n",
    "\n",
    "# Sample a subset of users for visualization\n",
    "sample_size_users = 1000\n",
    "if num_users > sample_size_users:\n",
    "    np.random.seed(42)\n",
    "    sample_indices_users = np.random.choice(num_users, sample_size_users, replace=False)\n",
    "    U_pca_sample = U_pca[sample_indices_users]\n",
    "else:\n",
    "    U_pca_sample = U_pca\n",
    "\n",
    "# Plot User Embeddings\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(U_pca_sample[:,0], U_pca_sample[:,1], alpha=0.5, s=10, color='blue')\n",
    "plt.title('User Embeddings Visualization (PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('user_embeddings.pdf')\n",
    "plt.close()\n",
    "print(\"User embeddings plot saved as 'user_embeddings.pdf'.\")\n",
    "\n",
    "# Perform PCA on item embeddings (excluding genre features)\n",
    "print(\"Performing PCA on item embeddings...\")\n",
    "V_only = V_feat[:, num_genres:]\n",
    "pca_items = PCA(n_components=2)\n",
    "V_pca = pca_items.fit_transform(V_only)\n",
    "\n",
    "# Sample a subset of items for visualization\n",
    "sample_size_items = 1000\n",
    "if num_movies > sample_size_items:\n",
    "    np.random.seed(42)\n",
    "    sample_indices_items = np.random.choice(num_movies, sample_size_items, replace=False)\n",
    "    V_pca_sample = V_pca[sample_indices_items]\n",
    "    genres_sample = genre_features[sample_indices_items]\n",
    "    # Assign a primary genre for coloring\n",
    "    primary_genres = np.argmax(genres_sample, axis=1)\n",
    "else:\n",
    "    V_pca_sample = V_pca\n",
    "    genres_sample = genre_features\n",
    "    primary_genres = np.argmax(genres_sample, axis=1)\n",
    "\n",
    "# Create a color palette\n",
    "unique_genres_sample = np.unique(primary_genres)\n",
    "palette = sns.color_palette(\"hsv\", len(unique_genres_sample))\n",
    "\n",
    "# Plot Item Embeddings with Genre Colors\n",
    "plt.figure(figsize=(8,6))\n",
    "for genre in unique_genres_sample:\n",
    "    idx = primary_genres == genre\n",
    "    plt.scatter(V_pca_sample[idx,0], V_pca_sample[idx,1], \n",
    "                alpha=0.5, s=10, label=genre_feature_names[genre], color=palette[genre])\n",
    "plt.title('Item Embeddings Visualization (PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Genres', bbox_to_anchor=(1.05, 1), loc='upper left', markerscale=2, fontsize='small')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('item_embeddings.pdf')\n",
    "plt.close()\n",
    "print(\"Item embeddings plot saved as 'item_embeddings.pdf'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def evaluate_model(params):\n",
    "    num_factors = params['num_factors']\n",
    "    lambda_reg = params['lambda_reg']\n",
    "    num_iters = params['num_iters']\n",
    "    \n",
    "    # Initialize model parameters\n",
    "    U_temp = np.random.normal(scale=0.1, size=(num_users, num_factors)).astype(np.float32)\n",
    "    V_temp = np.random.normal(scale=0.1, size=(num_movies, num_factors)).astype(np.float32)\n",
    "    user_bias_temp = np.zeros(num_users, dtype=np.float32)\n",
    "    item_bias_temp = np.zeros(num_movies, dtype=np.float32)\n",
    "    \n",
    "    # Train the model\n",
    "    for it in range(num_iters):\n",
    "        user_bias_temp = update_user_biases(\n",
    "            num_users, max_user_rated, user_rated_items_np, user_rated_ratings_np,\n",
    "            global_mean, item_bias_temp, U_temp, V_temp, lambda_reg, num_factors\n",
    "        )\n",
    "        item_bias_temp = update_item_biases(\n",
    "            num_movies, max_item_rated, item_rated_by_users_np, item_rated_ratings_np,\n",
    "            global_mean, user_bias_temp, U_temp, V_temp, lambda_reg, num_factors\n",
    "        )\n",
    "        U_temp = update_U(\n",
    "            num_users, max_user_rated, user_rated_items_np, user_rated_ratings_np,\n",
    "            global_mean, user_bias_temp, V_temp, U_temp, lambda_reg, num_factors\n",
    "        )\n",
    "        V_temp = update_V(\n",
    "            num_movies, max_item_rated, item_rated_by_users_np, item_rated_ratings_np,\n",
    "            global_mean, item_bias_temp, U_temp, V_temp, lambda_reg, num_factors\n",
    "        )\n",
    "    \n",
    "    # Compute RMSE on test set\n",
    "    rmse = compute_rmse_test(\n",
    "        test_users, test_items, test_ratings,\n",
    "        global_mean, user_bias_temp, item_bias_temp, U_temp, V_temp\n",
    "    )\n",
    "    \n",
    "    return rmse, params\n",
    "\n",
    "# Define a list of hyperparameter combinations\n",
    "param_grid = [\n",
    "    {'num_factors': 20, 'lambda_reg': 0.1, 'num_iters': 10},\n",
    "    {'num_factors': 30, 'lambda_reg': 0.05, 'num_iters': 20},\n",
    "    # Add more combinations as needed\n",
    "]\n",
    "\n",
    "# Parallel evaluation\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(evaluate_model)(params) for params in param_grid\n",
    ")\n",
    "\n",
    "# Find the best parameters\n",
    "best_rmse = float('inf')\n",
    "best_params = {}\n",
    "for rmse, params in results:\n",
    "    print(f\"Parameters: {params} => Test RMSE: {rmse:.4f}\")\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best RMSE: {best_rmse:.4f} with parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Hyperparameters in ALS\n",
    "a. Number of Latent Factors (num_factors)\n",
    "Definition: The dimensionality of the latent feature space for both users and items.\n",
    "Impact:\n",
    "Too Low: The model may be too simplistic, failing to capture underlying patterns, leading to underfitting.\n",
    "Too High: The model may capture noise, leading to overfitting and increased computational cost.\n",
    "b. Regularization Parameter (lambda_reg)\n",
    "Definition: Controls the extent to which the model penalizes large weights in the latent factor matrices.\n",
    "Impact:\n",
    "High Value: Prevents overfitting by discouraging complex models but might lead to underfitting.\n",
    "Low Value: Allows the model to fit the training data more closely but risks overfitting.\n",
    "c. Number of Iterations (num_iters)\n",
    "Definition: The number of times the ALS algorithm iteratively updates user and item factors.\n",
    "Impact:\n",
    "Too Few: The model may not converge, leading to suboptimal performance.\n",
    "Too Many: Increases computation time with diminishing returns once convergence is achieved.\n",
    "d. Early Stopping Criteria (Optional)\n",
    "Definition: A condition to halt training early if the model stops improving.\n",
    "Impact: Saves computational resources and prevents overfitting.\n",
    "e. Feature-Related Parameters (if using features)\n",
    "Definition: Parameters related to the incorporation of additional features (e.g., genre weights).\n",
    "Impact: Enhances the model's ability to handle cold start problems but adds complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# -----------------------------\n",
    "# 1. ALS Model Functions\n",
    "# -----------------------------\n",
    "\n",
    "def initialize_als_model(num_users, num_items, num_factors=20, lambda_reg=0.1, down_weight_bias=False):\n",
    "    \"\"\"\n",
    "    Initialize ALS model parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - num_users: Total number of users.\n",
    "    - num_items: Total number of items.\n",
    "    - num_factors: Number of latent factors.\n",
    "    - lambda_reg: Regularization parameter.\n",
    "    - down_weight_bias: If True, apply a tweak to down-weigh item biases.\n",
    "\n",
    "    Returns:\n",
    "    - user_bias: User bias vector.\n",
    "    - item_bias: Item bias vector.\n",
    "    - U: User latent factor matrix.\n",
    "    - V: Item latent factor matrix.\n",
    "    - down_weight_bias: Boolean flag for model version.\n",
    "    \"\"\"\n",
    "    user_bias = np.zeros(num_users, dtype=np.float32)\n",
    "    item_bias = np.zeros(num_items, dtype=np.float32)\n",
    "    U = np.random.normal(scale=0.1, size=(num_users, num_factors)).astype(np.float32)\n",
    "    V = np.random.normal(scale=0.1, size=(num_items, num_factors)).astype(np.float32)\n",
    "    return user_bias, item_bias, U, V, down_weight_bias\n",
    "\n",
    "def train_als(user_bias, item_bias, U, V, train_data, num_users, num_items, num_factors=20, lambda_reg=0.1, num_iters=10, down_weight_bias=False):\n",
    "    \"\"\"\n",
    "    Train ALS model by updating biases and latent factors.\n",
    "\n",
    "    Parameters:\n",
    "    - user_bias: User bias vector.\n",
    "    - item_bias: Item bias vector.\n",
    "    - U: User latent factor matrix.\n",
    "    - V: Item latent factor matrix.\n",
    "    - train_data: List of tuples (user_idx, item_idx, rating).\n",
    "    - num_users: Total number of users.\n",
    "    - num_items: Total number of items.\n",
    "    - num_factors: Number of latent factors.\n",
    "    - lambda_reg: Regularization parameter.\n",
    "    - num_iters: Number of iterations.\n",
    "    - down_weight_bias: If True, apply a tweak to down-weigh item biases.\n",
    "\n",
    "    Returns:\n",
    "    - Updated user_bias, item_bias, U, V\n",
    "    \"\"\"\n",
    "    for it in range(num_iters):\n",
    "        # Update user biases\n",
    "        for u in range(num_users):\n",
    "            items_rated_by_u = [item for (user, item, _) in train_data if user == u]\n",
    "            if not items_rated_by_u:\n",
    "                continue\n",
    "            sum_ratings = sum([rating for (user, item, rating) in train_data if user == u])\n",
    "            sum_biases = sum([item_bias[i] for i in items_rated_by_u])\n",
    "            U_u = U[u]\n",
    "            sum_latent = sum([np.dot(U_u, V[i]) for i in items_rated_by_u])\n",
    "            user_bias[u] = (sum_ratings - sum_biases - sum_latent) / (len(items_rated_by_u) + lambda_reg)\n",
    "        \n",
    "        # Update item biases\n",
    "        for i in range(num_items):\n",
    "            users_who_rated_i = [user for (user, item, _) in train_data if item == i]\n",
    "            if not users_who_rated_i:\n",
    "                continue\n",
    "            sum_ratings = sum([rating for (user, item, rating) in train_data if item == i])\n",
    "            sum_biases = sum([user_bias[u] for u in users_who_rated_i])\n",
    "            V_i = V[i]\n",
    "            sum_latent = sum([np.dot(U[u], V_i) for u in users_who_rated_i])\n",
    "            item_bias[i] = (sum_ratings - sum_biases - sum_latent) / (len(users_who_rated_i) + lambda_reg)\n",
    "        \n",
    "        # Update user latent factors U\n",
    "        for u in range(num_users):\n",
    "            items_rated_by_u = [item for (user, item, _) in train_data if user == u]\n",
    "            if not items_rated_by_u:\n",
    "                continue\n",
    "            A = np.zeros((num_factors, num_factors), dtype=np.float32)\n",
    "            b = np.zeros(num_factors, dtype=np.float32)\n",
    "            for i in items_rated_by_u:\n",
    "                V_i = V[i]\n",
    "                A += np.outer(V_i, V_i) + lambda_reg * np.eye(num_factors, dtype=np.float32)\n",
    "                b += (train_data[[idx for idx, d in enumerate(train_data) if d[0]==u and d[1]==i][0]][2] - user_bias[u] - item_bias[i]) * V_i\n",
    "            U[u] = np.linalg.solve(A, b)\n",
    "        \n",
    "        # Update item latent factors V\n",
    "        for i in range(num_items):\n",
    "            users_who_rated_i = [user for (user, item, _) in train_data if item == i]\n",
    "            if not users_who_rated_i:\n",
    "                continue\n",
    "            A = np.zeros((num_factors, num_factors), dtype=np.float32)\n",
    "            b = np.zeros(num_factors, dtype=np.float32)\n",
    "            for u in users_who_rated_i:\n",
    "                U_u = U[u]\n",
    "                A += np.outer(U_u, U_u) + lambda_reg * np.eye(num_factors, dtype=np.float32)\n",
    "                b += (train_data[[idx for idx, d in enumerate(train_data) if d[0]==u and d[1]==i][0]][2] - user_bias[u] - item_bias[i]) * U_u\n",
    "            V[i] = np.linalg.solve(A, b)\n",
    "        \n",
    "        # Apply tweak for Version B if needed\n",
    "        if down_weight_bias:\n",
    "            item_bias *= 0.8  # Down-weigh item biases by 20%\n",
    "        \n",
    "        print(f\"Iteration {it+1}/{num_iters} completed.\")\n",
    "    \n",
    "    return user_bias, item_bias, U, V\n",
    "\n",
    "def predict_rating(user_idx, item_idx, user_bias, item_bias, U, V, down_weight_bias=False):\n",
    "    \"\"\"\n",
    "    Predict rating for a given user and item.\n",
    "\n",
    "    Parameters:\n",
    "    - user_idx: Index of the user.\n",
    "    - item_idx: Index of the item.\n",
    "    - user_bias: User bias vector.\n",
    "    - item_bias: Item bias vector.\n",
    "    - U: User latent factor matrix.\n",
    "    - V: Item latent factor matrix.\n",
    "    - down_weight_bias: If True, apply a tweak to down-weigh item biases.\n",
    "\n",
    "    Returns:\n",
    "    - Predicted rating.\n",
    "    \"\"\"\n",
    "    bias = user_bias[user_idx] + item_bias[item_idx]\n",
    "    latent = np.dot(U[user_idx], V[item_idx])\n",
    "    if down_weight_bias:\n",
    "        bias *= 0.8  # Down-weight item biases by 20%\n",
    "    return bias + latent\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Recommendation Function\n",
    "# -----------------------------\n",
    "\n",
    "def get_candidate_items(user_id, num_items=100):\n",
    "    \"\"\"\n",
    "    Retrieve candidate items for recommendation.\n",
    "\n",
    "    Parameters:\n",
    "    - user_id: ID of the user.\n",
    "    - num_items: Number of candidate items to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    - List of dictionaries with 'item_id' and 'item_idx'.\n",
    "    \"\"\"\n",
    "    # Placeholder: In practice, retrieve based on user history or item popularity\n",
    "    return [{'item_id': i, 'item_idx': i} for i in range(1, num_items + 1)]\n",
    "\n",
    "def createreco(user_id, version_id, models, num_users, num_items):\n",
    "    \"\"\"\n",
    "    Generate a list of recommended items for a user based on the model version.\n",
    "\n",
    "    Parameters:\n",
    "    - user_id: Unique identifier for the user.\n",
    "    - version_id: 'A' for control or 'B' for variant.\n",
    "    - models: Dictionary containing model versions.\n",
    "    - num_users: Total number of users.\n",
    "    - num_items: Total number of items.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples containing (item_id, item_title).\n",
    "    \"\"\"\n",
    "    model = models.get(version_id)\n",
    "    if model is None:\n",
    "        raise ValueError(f\"Invalid version_id: {version_id}\")\n",
    "    \n",
    "    user_idx = user_id - 1  # Assuming user IDs start at 1\n",
    "    candidate_items = get_candidate_items(user_id, num_items=num_items)\n",
    "    recommendations = []\n",
    "    \n",
    "    for item in candidate_items:\n",
    "        item_id = item['item_id']\n",
    "        item_idx = item['item_idx'] - 1  # Assuming item IDs start at 1\n",
    "        pred_rating = predict_rating(user_idx, item_idx, model['user_bias'], model['item_bias'], model['U'], model['V'], model['down_weight_bias'])\n",
    "        recommendations.append((item_id, f\"Item {item_id}\", pred_rating))\n",
    "    \n",
    "    # Sort by predicted rating descending\n",
    "    recommendations.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Return top 10 recommendations\n",
    "    top_n = 10\n",
    "    return [(item_id, title) for item_id, title, _ in recommendations[:top_n]]\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Logging Function\n",
    "# -----------------------------\n",
    "\n",
    "LOG_FILE = 'feedback_log.csv'\n",
    "\n",
    "# Initialize the CSV file with headers if it doesn't exist\n",
    "if not os.path.exists(LOG_FILE):\n",
    "    with open(LOG_FILE, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['timestamp', 'user_id', 'item_id', 'feedback', 'version_id'])\n",
    "\n",
    "def log_feedback(user_id, item_id, feedback, version_id):\n",
    "    \"\"\"\n",
    "    Log user feedback to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - user_id: Unique identifier for the user.\n",
    "    - item_id: Unique identifier for the item.\n",
    "    - feedback: User's feedback on the item (e.g., rating).\n",
    "    - version_id: 'A' or 'B' indicating the model version used.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.utcnow().isoformat()\n",
    "    with open(LOG_FILE, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([timestamp, user_id, item_id, feedback, version_id])\n",
    "\n",
    "# -----------------------------\n",
    "# 4. User Assignment\n",
    "# -----------------------------\n",
    "\n",
    "def assign_users_to_groups(user_ids, seed=42):\n",
    "    \"\"\"\n",
    "    Randomly assign users to group A or B.\n",
    "\n",
    "    Parameters:\n",
    "    - user_ids: List of user IDs.\n",
    "    - seed: Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary mapping user IDs to group ('A' or 'B').\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    user_group = {}\n",
    "    for user_id in user_ids:\n",
    "        group = random.choice(['A', 'B'])\n",
    "        user_group[user_id] = group\n",
    "    return user_group\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Simulate Feedback\n",
    "# -----------------------------\n",
    "\n",
    "def simulate_feedback(recommendations, true_preferences):\n",
    "    \"\"\"\n",
    "    Simulate user feedback based on true preferences.\n",
    "\n",
    "    Parameters:\n",
    "    - recommendations: List of recommended items (item_id, item_title).\n",
    "    - true_preferences: Dictionary mapping item_id to user's true preference score.\n",
    "\n",
    "    Returns:\n",
    "    - List of simulated feedback scores.\n",
    "    \"\"\"\n",
    "    feedback = []\n",
    "    for item_id, _ in recommendations:\n",
    "        true_pref = true_preferences.get(item_id, 3.0)  # Default preference is neutral\n",
    "        noise = np.random.normal(0, 0.5)  # Gaussian noise\n",
    "        simulated_rating = np.clip(true_pref + noise, 1.0, 5.0)  # Ratings between 1 and 5\n",
    "        feedback.append(simulated_rating)\n",
    "    return feedback\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Run A/B Test\n",
    "# -----------------------------\n",
    "\n",
    "def run_ab_test(user_group_mapping, models, true_preferences_mapping, num_users, num_items):\n",
    "    \"\"\"\n",
    "    Run A/B test by generating recommendations, simulating feedback, and logging interactions.\n",
    "\n",
    "    Parameters:\n",
    "    - user_group_mapping: Dictionary mapping user IDs to group ('A' or 'B').\n",
    "    - models: Dictionary mapping version IDs to model parameters.\n",
    "    - true_preferences_mapping: Dictionary mapping user IDs to their true preferences (dicts).\n",
    "    - num_users: Total number of users.\n",
    "    - num_items: Total number of items.\n",
    "    \"\"\"\n",
    "    for user_id, group in user_group_mapping.items():\n",
    "        recommendations = createreco(user_id, group, models, num_users, num_items)\n",
    "        true_prefs = true_preferences_mapping.get(user_id, {})\n",
    "        feedback = simulate_feedback(recommendations, true_prefs)\n",
    "        for (item_id, _), fb in zip(recommendations, feedback):\n",
    "            log_feedback(user_id, item_id, fb, group)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Evaluation Functions\n",
    "# -----------------------------\n",
    "\n",
    "def evaluate_ab_test(log_file):\n",
    "    \"\"\"\n",
    "    Evaluate A/B test results by comparing feedback metrics between groups A and B.\n",
    "\n",
    "    Parameters:\n",
    "    - log_file: Path to the CSV file containing logged feedback.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing evaluation metrics for both groups.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(log_file)\n",
    "    group_A = df[df['version_id'] == 'A']\n",
    "    group_B = df[df['version_id'] == 'B']\n",
    "    \n",
    "    metrics = {}\n",
    "    for group, data in zip(['A', 'B'], [group_A, group_B]):\n",
    "        metrics[group] = {}\n",
    "        metrics[group]['mean_rating'] = data['feedback'].mean()\n",
    "        metrics[group]['std_rating'] = data['feedback'].std()\n",
    "        metrics[group]['count'] = data.shape[0]\n",
    "        metrics[group]['rating_distribution'] = data['feedback'].value_counts(normalize=True).sort_index()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def compare_groups(metrics, log_file):\n",
    "    \"\"\"\n",
    "    Compare performance metrics between groups A and B.\n",
    "\n",
    "    Parameters:\n",
    "    - metrics: Dictionary containing evaluation metrics for both groups.\n",
    "    - log_file: Path to the CSV file containing logged feedback.\n",
    "    \"\"\"\n",
    "    print(\"A/B Test Evaluation Results:\")\n",
    "    for group in ['A', 'B']:\n",
    "        print(f\"\\nGroup {group}:\")\n",
    "        print(f\"  Number of Feedbacks: {metrics[group]['count']}\")\n",
    "        print(f\"  Mean Rating: {metrics[group]['mean_rating']:.2f}\")\n",
    "        print(f\"  Std Rating: {metrics[group]['std_rating']:.2f}\")\n",
    "        print(f\"  Rating Distribution:\")\n",
    "        print(metrics[group]['rating_distribution'])\n",
    "    \n",
    "    # Perform t-test to see if differences are significant\n",
    "    df = pd.read_csv(log_file)\n",
    "    ratings_A = df[df['version_id'] == 'A']['feedback']\n",
    "    ratings_B = df[df['version_id'] == 'B']['feedback']\n",
    "    \n",
    "    t_stat, p_val = ttest_ind(ratings_A, ratings_B, equal_var=False)\n",
    "    print(\"\\nStatistical Test (Independent t-test):\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_val:.4f}\")\n",
    "    \n",
    "    if p_val < 0.05:\n",
    "        print(\"  Conclusion: Significant difference between Group A and B.\")\n",
    "    else:\n",
    "        print(\"  Conclusion: No significant difference between Group A and B.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Example Workflow\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    # Parameters\n",
    "    num_users = 10  # Example: 10 users\n",
    "    num_items = 100  # Example: 100 items\n",
    "    num_factors = 20\n",
    "    lambda_reg = 0.1\n",
    "    num_iters = 5  # Reduced for demonstration purposes\n",
    "    \n",
    "    # Step 1: Initialize Models\n",
    "    user_bias_A, item_bias_A, U_A, V_A, down_weight_bias_A = initialize_als_model(num_users, num_items, num_factors, lambda_reg, down_weight_bias=False)\n",
    "    user_bias_B, item_bias_B, U_B, V_B, down_weight_bias_B = initialize_als_model(num_users, num_items, num_factors, lambda_reg, down_weight_bias=True)\n",
    "    \n",
    "    # Step 2: Prepare Training Data (Placeholder)\n",
    "    # In practice, replace this with your actual training data\n",
    "    # Here, we simulate some training data\n",
    "    train_data = []\n",
    "    for u in range(num_users):\n",
    "        for i in range(num_items):\n",
    "            rating = np.random.uniform(1, 5)  # Random rating between 1 and 5\n",
    "            train_data.append((u, i, rating))\n",
    "    \n",
    "    # Step 3: Train Models\n",
    "    user_bias_A, item_bias_A, U_A, V_A = train_als(user_bias_A, item_bias_A, U_A, V_A, train_data, num_users, num_items, num_factors, lambda_reg, num_iters, down_weight_bias=False)\n",
    "    user_bias_B, item_bias_B, U_B, V_B = train_als(user_bias_B, item_bias_B, U_B, V_B, train_data, num_users, num_items, num_factors, lambda_reg, num_iters, down_weight_bias=True)\n",
    "    \n",
    "    # Step 4: Define Models Dictionary\n",
    "    models = {\n",
    "        'A': {\n",
    "            'user_bias': user_bias_A,\n",
    "            'item_bias': item_bias_A,\n",
    "            'U': U_A,\n",
    "            'V': V_A,\n",
    "            'down_weight_bias': False\n",
    "        },\n",
    "        'B': {\n",
    "            'user_bias': user_bias_B,\n",
    "            'item_bias': item_bias_B,\n",
    "            'U': U_B,\n",
    "            'V': V_B,\n",
    "            'down_weight_bias': True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Step 5: Assign Users to Groups\n",
    "    all_user_ids = list(range(1, num_users + 1))  # User IDs from 1 to num_users\n",
    "    user_group_mapping = assign_users_to_groups(all_user_ids, seed=42)\n",
    "    \n",
    "    # Step 6: Define True Preferences for Simulation\n",
    "    # For simulation, assign some true preferences to users\n",
    "    true_preferences_mapping = {\n",
    "        1: {1: 5.0, 2: 3.0, 3: 4.0},\n",
    "        2: {4: 2.0, 5: 5.0, 6: 3.5},\n",
    "        3: {7: 4.5, 8: 2.5, 9: 3.0},\n",
    "        4: {10: 3.0, 11: 4.0, 12: 2.5},\n",
    "        5: {13: 5.0, 14: 3.5, 15: 4.5},\n",
    "        6: {16: 2.5, 17: 4.0, 18: 3.0},\n",
    "        7: {19: 3.5, 20: 2.0, 21: 4.5},\n",
    "        8: {22: 4.0, 23: 3.0, 24: 5.0},\n",
    "        9: {25: 1.5, 26: 4.0, 27: 3.5},\n",
    "        10: {28: 2.0, 29: 3.5, 30: 4.0}\n",
    "    }\n",
    "    \n",
    "    # Step 7: Run A/B Test\n",
    "    run_ab_test(user_group_mapping, models, true_preferences_mapping, num_users, num_items)\n",
    "    \n",
    "    # Step 8: Evaluate Results\n",
    "    metrics = evaluate_ab_test(LOG_FILE)\n",
    "    compare_groups(metrics, LOG_FILE)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python ab_test_als.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
